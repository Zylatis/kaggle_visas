goal: make a dashboard to help people maximize their chances of getting visa (things they can change, things they cant')
i.e. predict outcome then optimize

- 370k datapoints, mostly certifed - what to do with withdrawn? Come back to
- Lots of missing data (reduces 150 cols to about 15)
	- requires some cleaning, re arranging (yearly vs hourly salary, ignoring some columns like ID, fixing string errors)
	- Ignore columns which have more than 90% missing data, then drop rows which lack data from remaining set
	
- Possible options: Tree, regression, SVM, NN. Start with trees

- Dealing with catagoricals, one hot vs label
- Normalize data with scikit learn

- With 1 hot encoding very difficult to beat false positive rate, also have massive memory issues

- Adding in some meta data (% of that country approved, replacing date with continuous variable) doesn't seem to help

- Comparing to other kernels most grossly underestimate the false positive rate, or use mode to fill in missing data along with normal label encoding

- Using label encoding gets around the memory problem and so can test whole set, but still unable to beat false positive case
	- still todo: grid search and NN, may fare better, maybe not
	
- Summary: insufficient data to model accurately.


feature selection/correlation
PCA
class weight = balanced scikitlearn 
feature leak split
flask app
